# Logistic Regression for Classification Tasks

In this repository, you can find two different classification tasks using logistic regression. The first one is performed on the Titanic dataset from Kaggle, while the second one uses a fake advertising dataset.

## Titanic Dataset

The Titanic dataset contains information about passengers on the Titanic and whether or not they survived. The goal of this classification task is to predict whether a passenger would survive based on their features.

I the first part of the Jupyter notebook 'classification_task.ipynb' the code for loading, exploring, preprocessing, training, and evaluating a logistic regression model on the Titanic dataset is present. The notebook includes comments explaining each step and visualizations to help understand the data.
Fake Advertising Dataset

The Fake Advertising dataset contains information about people who have seen an advertisement and whether or not they clicked on it. The goal of this classification task is to predict whether a person will click on an advertisement based on their features. The features are as follows:

* 'Daily Time Spent on Site': consumer time on site in minutes
* 'Age': cutomer age in years
* 'Area Income': Avg. Income of geographical area of consumer
* 'Daily Internet Usage': Avg. minutes a day consumer is on the internet
* 'Ad Topic Line': Headline of the advertisement
* 'City': City of consumer
* 'Male': Whether or not consumer was male
* 'Country': Country of consumer
* 'Timestamp': Time at which consumer clicked on Ad or closed window
* 'Clicked on Ad': 0 or 1 indicated clicking on Ad

In the second part of the Jupyter notebook 'classification_task.ipynb', the code for loading, exploring, preprocessing, training, and evaluating a logistic regression model on the Fake Advertising dataset is present. The notebook includes comments explaining each step and visualizations to help understand the data.

## Conclusion

Logistic regression is a powerful tool for classification tasks, and this repository provides examples of using it on two different datasets. By exploring and preprocessing the data and training and evaluating the model, we can gain insights into the factors that affect the outcome and build accurate predictive models.